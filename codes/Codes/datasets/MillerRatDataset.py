import numpy as np
from .BaseTwoStepDataset import BaseTwoStepDataset, _combine_data_dict, _segment_long_block
import joblib


class MillerRatDataset(BaseTwoStepDataset):
    """A dataset class for the standard two-step task with binary actions, states, and rewards.

    Miller's rats.
    Load pre-processed data generated from his Matlab code.

    Attributes:
         unique_trial_type: How many possible unique trial observations (actions * states * rewards = 8 combinations)
         behav: Standard format of behavioral data.
         data_path: Where to load the data.
         behav_format: tensor (for RNN) or cog_session (for Cog agents)?
         torch_beahv_input: tensor format of agent's input
         torch_beahv_input_1hot: tensor format of agent's input (one-hot encoding of trial observations)
         torch_beahv_target: tensor format of agent's target output
         cog_sessions: cog_session format of agent's input & target output
         batch_size: How many blocks are there in the current loaded data?
    """
    def __init__(self, data_path=None, behav_data_spec=None, neuro_data_spec=None, verbose=True):
        """Initialize the dataset. """
        self._data_dict = joblib.load(data_path / 'rat_data_dict.pkl')
        self.unique_trial_type = 8
        self.sessions = {
                'm70': ['2014-11-06', '2014-11-07', '2014-11-10', '2014-11-11', '2014-11-13', '2014-11-14', '2014-11-18',
                            '2014-11-19', '2014-12-04', '2014-12-08', '2014-12-10', '2014-12-11', '2014-12-16','2014-12-17'],
                'm55': ['2015-05-13','2015-05-14','2015-05-19','2015-05-26','2015-05-28','2015-06-01','2015-06-02','2015-06-03',
                            '2015-06-04','2015-06-08','2015-06-09','2015-06-10','2015-06-11','2015-06-12','2015-06-19'],
                'm64':['2016-01-06', '2016-01-07', '2016-01-08', '2016-01-13', '2016-01-21',
                       '2016-01-25', '2016-02-08', '2016-02-10', '2016-02-16', '2016-02-17'],
            'm71': ['2015-03-04','2015-03-11','2015-03-20','2015-03-26','2015-03-30','2015-04-13'],
            'm88': ['2016-07-18','2016-07-19','2016-08-01','2016-08-03','2016-09-02','2016-09-05'],
            }
        self.sessions['all'] = [sess for animal in self.sessions for sess in self.sessions[animal]]
        super().__init__(data_path, behav_data_spec, neuro_data_spec, verbose=verbose)
        # _data_dict[date]['spike_ensemble_0/1/2/3']: shape = (n_neurons, n_max_trials, n_time_bins)
        # step one initiation (240 bin, -12s~12s, 0.1s/bin)
        # choice port entry (240 bin, -12s~12s, 0.1s/bin)
        # step two initiation (240 bin, -12s~12s, 0.1s/bin)
        # outcome port entry (240 bin, -12s~12s, 0.1s/bin)
        # mat file generated by ofc-learning-choosing-paper\processing_scripts\bin_organize_ephys_data.m
        # processed by twostep_exp\extract_ofc_data.py

    def _detect_trial_type(self):
        """Detect trial type from behavioral data."""
        behav = self.behav
        behav['trial_type'] = []
        for i in range(len(behav['action'])):
            behav['trial_type'].append(behav['action'][i] * 4 + behav['stage2'][i] * 2 + behav['reward'][i])

    def _load_all_trial_type(self, behav_data_spec):
        np2list = lambda L: [np.array([x]) for x in L]
        self.behav = {
            'action': np2list([0,0,0,0,1,1,1,1]),
            'stage2': np2list([0,0,1,1,0,0,1,1]),
            'reward': np2list([0,1,0,1,0,1,0,1]),
        }
        self._detect_trial_type()

    def load_data(self, behav_data_spec, neuro_data_spec=None, verbose=True):
        """Load data from disk following data specifications. """
        if 'all_trial_type' in behav_data_spec and behav_data_spec['all_trial_type']:
            self._load_all_trial_type(behav_data_spec)
            return
        if 'animal_name' in behav_data_spec and 'session_name' not in behav_data_spec:
            behav_data_spec['session_name'] = self.sessions[behav_data_spec['animal_name']]
        if isinstance(behav_data_spec['session_name'], str):
            behav_data_spec['session_name'] = [behav_data_spec['session_name']]
        if neuro_data_spec is None:
            neuro_data_spec = {}
        self.behav_data_spec = behav_data_spec
        self.neuro_data_spec = neuro_data_spec

        self.behav = behav = {}
        self.neuro = neuro = {}
        for sess_name in behav_data_spec['session_name']:
            dt = self._data_dict[sess_name]
            nTrials = dt['nTrials']
            if 'block_truncation' in behav_data_spec:
                bt = behav_data_spec['block_truncation']
                if nTrials < bt[1]:
                    if verbose: print(f'WARNING: session {sess_name} have {nTrials} trials fewer than {bt[1]}; IGNORED')
                    continue
                eff_trials = np.arange(bt[0], bt[1])
            else:
                eff_trials = np.arange(0, nTrials)
            n_eff_trials = len(eff_trials)
            if verbose: print('====Loaded session', sess_name, 'trial num', len(eff_trials),end=',')
            if 'max_segment_length' in behav_data_spec:
                max_segment_length = behav_data_spec['max_segment_length']
            else:
                max_segment_length = None
            action_segments = _segment_long_block(np.where(dt['sides1'][eff_trials] == 'l', 0, 1), max_trial_num=max_segment_length, verbose=verbose)
            if dt['p_congruent'] > 0.5:
                stage2_often = 'l' # for congruent trials, l action leads to l state often
            else:
                stage2_often = 'r' # for incongruent trials, l action leads to r state often
            stage2_segments = _segment_long_block(np.where(dt['sides2'][eff_trials] == stage2_often, 0, 1), max_trial_num=max_segment_length)
            reward_segments = _segment_long_block((dt['rewards'][eff_trials, 0]), max_trial_num=max_segment_length)
            if verbose: print('')
            behav.setdefault('action', []).extend(action_segments) # list of 1d array
            behav.setdefault('stage2', []).extend(stage2_segments)
            behav.setdefault('reward', []).extend(reward_segments)
            if len(neuro_data_spec) > 0:
                n_events = 4
                spike_ensemble = np.array([dt[f'spike_ensemble_{i}'] for i in range(n_events)]) # shape = (n_events, n_neurons, n_trials_max, n_time_bins)?
                spike_ensemble = spike_ensemble[:, :, eff_trials, :] # shape = (n_events, n_neurons, n_eff_trials, n_time_bins)
                spike_ensemble = spike_ensemble.transpose(2, 1, 0, 3) # shape = (n_eff_trials, n_neurons, n_events, n_time_bins)

                # extract time around events
                start_time_before_event = neuro_data_spec['start_time_before_event'] # e.g. -2 seconds
                end_time_after_event = neuro_data_spec['end_time_after_event'] # e.g. 4 seconds
                assert start_time_before_event >= -12 # -12 seconds
                assert end_time_after_event <= 12 # 12 seconds
                ori_bin_size = 0.1 # 0.1 seconds
                time_slice = slice(int((start_time_before_event + 12) / ori_bin_size), int((end_time_after_event + 12) / ori_bin_size))
                spike_ensemble = spike_ensemble[:, :, :, time_slice]

                # combine bins
                _, n_neurons, _, n_time_bins = spike_ensemble.shape
                final_bin_size = neuro_data_spec['bin_size'] # e.g. 0.2s
                assert final_bin_size % ori_bin_size == 0
                combined_bin_num = int(final_bin_size // ori_bin_size) # 2
                spike_ensemble = spike_ensemble.reshape(n_eff_trials, n_neurons, n_events, n_time_bins // combined_bin_num, combined_bin_num)
                spike_ensemble = spike_ensemble.sum(axis=-1) # shape = (n_eff_trials, n_neurons, n_events, n_time_bins // combined_bin_num)

                # segment long trials
                loc_start = 0
                for i, seg in enumerate(action_segments):
                    loc_end = loc_start + len(seg)
                    neuro.setdefault('session_name', []).append(sess_name)
                    neuro.setdefault('X', []).append(spike_ensemble[loc_start:loc_end])
                    loc_start = loc_end
                    # print('neuro', neuro['X'][-1].shape)

        self._detect_trial_type()
        if verbose: print('Total trial num:', self.total_trial_num)

    def get_neuro_data(self, session_name='', zcore=True, remove_nan=True, shape=3, **kwargs):
        """Get neural data for a session and block type.
        Args:
            session_name: str
            zcore: bool
            remove_nan: bool
            shape: int, 2 for (total_trial_num, feature_num), 3 for (episode_num, trial_num, feature_num)

        Returns:
            X: np.ndarray, shape (episode_num, trial_num, feature_num)
            episode_idx: np.ndarray, shape episode_num; indices for the selected episodes
            kept_feat_idx: np.ndarray, shape feature_num; indices for the selected features (non-nan)
            feat_scale: np.ndarray, shape feature_num; scale for each feature due to z-scoring
        """
        assert session_name in self.neuro['session_name']
        assert shape in [2, 3, 4]
        sess_idx = np.array(self.neuro['session_name']) == session_name
        episode_idx = np.where(sess_idx)[0]
        X = np.concatenate([self.neuro['X'][i] for i in episode_idx], axis=0)
        # replace nan with column mean
        col_mean = np.nanmean(X, axis=0)
        inds = np.where(np.isnan(X))
        X[inds] = np.take(col_mean, inds[1])

        total_trial_num, n_neurons, n_events, n_time_bins = X.shape
        feat_num = n_neurons * n_events * n_time_bins
        if zcore: # zscore for each feature
            from scipy import stats
            feat_scale = np.std(X.reshape((total_trial_num,feat_num)), axis=0)
            X = stats.zscore(X.reshape(total_trial_num,feat_num)).reshape(total_trial_num, n_neurons, n_events, n_time_bins)
        else:
            feat_scale = np.ones(feat_num)
        if remove_nan:
            assert shape == 2
            X = X.reshape(total_trial_num, feat_num)
            kept_feat_idx = ~np.isnan(X).any(axis=0)
            X = X[:, kept_feat_idx]  # remove nan columns
            feat_scale = feat_scale[kept_feat_idx]
            print('removed nan features:', np.sum(~kept_feat_idx), 'out of', feat_num)
            if feat_num == np.sum(~kept_feat_idx):
                print('all features are nan!')
                raise ValueError
        else:
            kept_feat_idx = np.ones(feat_num, dtype=bool)
        kept_feat_idx = kept_feat_idx.reshape((n_neurons, n_events, n_time_bins))
        if shape == 2:
            X = X.reshape(total_trial_num, -1)
        return X, episode_idx, kept_feat_idx, feat_scale